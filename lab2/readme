sin2x contains the approximation for the function sin(2x), 
The figures called sin2x_something are testruns for the sin2x approxaimtion with different number of hidden units.

square2x contains the approximation for the function square(2x).
The figures called square2x_something are testruns for the square2x approxaimtion with different number of hidden units.

First bunch of questions for sin2x:

â€¢ How many units did you require to get down to a maximum (absolute)
residual value of 0.1, 0.01 and 0.001?

Answer: 7,25,56

â€¢  Give a good reason for the big dierence in residual between 5 and 6 units
for sin(2x).

Answer: Hmm, probably something to do with the frequency of the function but why?
When using sin3x we need 8 units.

Second bunch of questions for square(2x).
â€¢ How many units did you require, when approximating square(2x),to come down to residual values of 0.1, 0.01 and 0.001?

Answer:59, 63, 63 (63 is the number of patterns we have to train on, that's why we get a good fit for it)

â€¢ Approximating square(2x) is a somewhat special case of function approximation since it is similar to another area of use for articial neural
networks. Which? Hint: ANNs can be used for pattern completion, noise reduction, . . .

Answer: Maybe restoring binary signals over networks? But it's not really a ANN field is it? Something similar though probably.
Need to look up uses for ANN:s.

â€¢ Can you, with a suitable action (e.g. transforming network output), easy
get down (for training values) to a residual value=0? What action? How
many units did you require?

Answer: We can threashold units so that they are either 1 or -1 with the sign function.
We require 6 hidden units. Hmm, why 6? We needed that for a good fit on the sin(2x) function aswell. Probably depends on the frequency of the wave?

â€¢ Can an RBF network solve the XOR problem? If not, explain why not.
If yes, explain how

Answer: Can an RBF make linear seperations? The lab notes seem to say that we can classify non lineraly seperable data with RBF:s, see page 3, top of page right before section 3.2.

testDiter and sampleDiter are for testing the delta rule of the RBF network.

diter questions:

â€¢ How many units and iterations did you require to come down to a maximum residual value of 0.01? 
What value(s) of Î· did you use?

Answer: A combination of 500000 iterations, eta = 0.05 and 50 units got us an error below 0.01. Perhaps try again with higher eta and lower iterations to make it faster. We could allasy use the same number of nodes as patterns ofcourse, but seems liek cheating.
Seems like this was too much though, but it was hard to get from 0.1 to 0.01

â€¢ Now try approximating some function of your own choice. Use least squares or the delta rule as you wish.

Answer: A linear function and a sin(x)+cos(x) function are approximated in the file ownfunction.m.
In the lienar function we see that the RBF works better for periodic functions.
